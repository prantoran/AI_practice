
state space
each iteration of the game is an episode
A terminal state is a state taht ends an episode (i.e. lose 100% health)
The duration of an episode is from the initial state to terminal state

episodic tasks
    gridworld, supermario

non-episodic/continuing tasks   
    controlling the room temperature

environment
    the world that the agent lives in
    an agent solves an env means that the trained agent can take more than some threshold reward.

policy
    a function that maps state to action (the agent's brain)
    representing a policy   
        can be function, equation or a neural netowrk
        can be probabilistic or deterministic

action space
    set of all possible actions


choosing rewards
- in gridworld: +1 winning state, -1 losing state
- 0 reward no matter what then the agent never learns that one action is better than another
    - -1 for each step
- rewarding the agent for using human strategies would be detrimental to discovering better novel strategies


The Markov Property
- language models
    - sentences are sequences of words, and words are states
    - probability model for sequences of words
- forget about any sequence longer than 2
    - bigram = sequence of length 2
    - p(quick| the), p(brown | quick) ...
- first-order markov assumption
    p(st | s(t-1), s(t-2), ..., s1) = p(st | s(t-1))
- basis of hidden markov models
    - speech recognition
- In deep q-learning for atari games
    - used screenshots
    - not just a single frame - they stacked 4 consecutive frames as state
- state transition matrix
    - maximum likelihood
    - for mdps, sth similar with more parameters
        - Aij = p(st = j | s(t-1) = i) ~ count(i->j) / count(i)
        - no time index, assuming relation independent of time


Markov Decision Processes (MDPS)
- a discrete-time stochastic control Processes
- environment
    - state
    - reward
    - S(t+1), R(t+1) ~ p(s', r | s, a)
- agent
    - action
- policy
    - At ~ œÄ(at | st)
- increment of timestamp is implicit in the env
- transition/step
    - {St, At, R(t+1), S(t+1)}
    - SARSA
        - {St, At, R(t+1), S(t+1), A(t+1)}
- both agent and env are probabilistic
- state-action diagram
    - action coming from a particular state
- environment dynamics
    - randomness of reward
        - deterministic: p(s' | s, a)
        - random: p(s', r | s, a)
    - randomness of next state
    - lack of perfect information
        - accuracy of sensors
        - incomplete/wrong information
        - in games where we can't see others' options 

|                          | States are fully observed     | States are partially unobserved  |
|:------------------------:|:-----------------------------:|:--------------------------------:|
| System is autonomous     | Markov Model                  | Hidden Markov Model (HMM)        |
| System is controlled     | Markov Decision Process (MDP) | Partially-Observable MDP (POMDP) |



Future rewards
- want to maximize sum of future rewards
- return
    - sum of future rewards
    - G(t) = R(t+1) + R(t+2) + ... 
- RL has planning built-in 
    - by maximizing the sum of future rewards,
        the agent comes up with whathe plan is necessary automatically
- discounting
    - de-evaluate further futuer rewards
    - ùõæ = 0 => greedy
    - ùõæ = 1 the true sum of future rewards
    - short episodic tasks should not be discounted
    - help break infinite loop

value function
- sum of future rewards is dependent on the policy
- return can be random  
    - not sure deterministically what our opponent will do
    - p(s', r | s, a)
- value function is expected return from a state s
- value of a terminal state is always zero
    - value func is the sum of expected future rewards

Bellman equation
    - game tree
        - each leaf node is a terminal state
        - depth of the tree is the duration
        - tre shows all possible trajectories
        - recursion: each child of a tree is a tree
    - G(t) = R(t+1) + ùõæG(t+1)
        - return is recursive
    - Law of total expectation
        - E[E[X|Y]] = E[X]
    - expectation is a linear operation
    - V(x) = E[R(t+1) + ùõæV(s') | St = s]        
    - solving the Bellman eqn is nothing but a system of linear euqations
        - if policy and env dynamics are known
        - feasible for small systems with few states
    - in order to learn, we might want to try performing new actions to improve our existing policy
        - new action not dictated by the current policy
    - stochastic policy
        - allows any action from a given state
            - sum of future rewards differnt based on what actions taken
    - 2 kinds of value functions
        - state-value: V(s) = E[G(t) | St = s]
        - action-value: Q(s, a) = E[G(t) | St = s, At = a]
            - Q-table
- State value V(s) is useful for evaluating a policy
    - Given a policy, what is the return I can expect?
- Action value Q(s, a) is useful for control    
    - I'm in state s, what is the best action to take?

Optimal policy and optimal value function
- a good policy is one that performs optimal behaviour
- for comparing policies, equailty must hold for all states in the state space
- the optimal value func is unique, the optimal policy is not
- if there were a better value func, then that would be the optimal one

Bellman optimality eqn
- take the best action
- optimal state-value vs optimal action-value
    - V*(s) = max_a(Q*(s, a))